<!doctypehtml><html class="theme-next pisces use-motion" lang=zh-Hans><meta charset=UTF-8><meta content=IE=edge http-equiv=X-UA-Compatible><meta content=width=device-width,initial-scale=1,maximum-scale=1 name=viewport><meta content=#222 name=theme-color><script src=/lib/pace/pace.min.js></script><link href=/lib/pace/.min.css?v=1.0.2 rel=stylesheet><meta content=no-transform http-equiv=Cache-Control><meta content=no-siteapp http-equiv=Cache-Control><link href=/lib/font-awesome/css/font-awesome.min.css?v=4.6.2 rel=stylesheet><link href=/css/main.css?v=6.0.0 rel=stylesheet><link href=/images/apple-touch-icon-next.png?v=6.0.0 rel=apple-touch-icon sizes=180x180><link href=/images/favicon-32x32-next.png?v=6.0.0 rel=icon sizes=32x32 type=image/png><link href=/images/favicon-16x16-next.png?v=6.0.0 rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg?v=6.0.0 rel=mask-icon><meta content="Hexo, NexT" name=keywords><meta content=website property=og:type><meta content=Reading-Paper property=og:title><meta content=https://johnnyzhu035.github.io/tags/index.html property=og:url><meta content="JohnnyZhu's HomePage!" property=og:site_name><meta property=og:locale><meta content=2024-07-18T09:23:47.000Z property=article:published_time><meta content=2024-07-18T09:38:55.907Z property=article:modified_time><meta content=Johnny property=article:author><meta content=summary name=twitter:card><script id=hexo.configurations>var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    version: '6.0.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };</script><link href=https://JohnnyZhu035.github.io/tags/ rel=canonical><title>JohnnyZhu's HomePage!</title><meta content="Hexo 7.3.0" name=generator><body itemscope itemtype=http://schema.org/WebPage lang=zh-Hans><div class="container sidebar-position-left page-home"><div class=headband></div><header class=header id=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-wrapper><div class=site-meta><div class=custom-logo-site-title><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <span class=site-title>JohnnyZhu's HomePage!</span> <span class=logo-line-after><i></i></span> </a></div><p class=site-subtitle>An undergraduate student in Southeast University Chien-Shiung Wu College</div><div class=site-nav-toggle><button><span class=btn-bar></span> <span class=btn-bar></span> <span class=btn-bar></span></button></div></div><nav class=site-nav><ul class=menu id=menu><li class="menu-item menu-item-home"><a href=/ rel=section> <i class="menu-item-icon fa fa-fw fa-home"></i> <br> 首页 </a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section> <i class="menu-item-icon fa fa-fw fa-tags"></i> <br> 标签 </a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section> <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br> 归档 </a><li class="menu-item menu-item-search"><a class=popup-trigger href=javascript:;> <i class="menu-item-icon fa fa-search fa-fw"></i> <br> 搜索 </a></ul><div class=site-search><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class=search-icon> <i class="fa fa-search"></i> </span><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span><div class=local-search-input-wrapper><input autocomplete=off id=local-search-input placeholder=搜索... spellcheck=false></div></div><div id=local-search-result></div></div></div></nav></div></header><main class=main id=main><div class=main-inner><div class=content-wrap><div class=content id=content><section class=posts-expand id=posts><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/22/Gaussian-Distribution/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/22/Gaussian-Distribution/ itemprop=url>Gaussian Distribution</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-22T16:46:46+08:00 title=创建于>2024-07-22</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-23T06:58:48+08:00 itemprop=dateModified title=更新于>2024-07-23</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/22/Gaussian-Distribution/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/22/Gaussian-Distribution/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>6 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=Why-Gaussian-Distribution—Central-Limit-Theorem><a title="Why Gaussian Distribution—Central Limit Theorem" class=headerlink href=#Why-Gaussian-Distribution—Central-Limit-Theorem></a>Why Gaussian Distribution—Central Limit Theorem</h2><h3 id=Population-and-Sample><a title="Population and Sample" class=headerlink href=#Population-and-Sample></a>Population and Sample</h3><p>The <em>population</em> refers to the whole set of individuals that we want to study,and the <em>sample</em> refers to the subset that we choose from the population to study(in respect for the fact that we can’t choose the whole population to study in a row)<h3 id=Central-Limit-Theorem><a title="Central Limit Theorem" class=headerlink href=#Central-Limit-Theorem></a>Central Limit Theorem</h3><p>The Central Limit Theorem (CLT) is a fundamental theorem in probability theory and statistics that states that the distribution of the sum (or average) of a large number of independent, identically distributed variables will be approximately normal, regardless of the underlying distribution.<p>More specifically, if we take a sample of $n$ observations from any population, calculate the sample’s mean, and repeat that process a large number of times, the distribution of those sample means will be normally distributed. This distribution will have the same mean as the population from which the samples were drawn, and its standard deviation (also known as the standard error) will be equal to the standard deviation of the population divided by the square root of the sample size.<h2 id=Single-Variable-Univariate-Gaussian-Distribution-Normal-Distribution><a title="Single Variable(Univariate) Gaussian Distribution(Normal Distribution)" class=headerlink href=#Single-Variable-Univariate-Gaussian-Distribution-Normal-Distribution></a>Single Variable(Univariate) Gaussian Distribution(Normal Distribution)</h2><h3 id=Definition><a class=headerlink href=#Definition title=Definition></a>Definition</h3><p>For a random variable $x$,if its <em>probability density function</em> is:<blockquote><p>$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$</blockquote><p>where $\mu$ and $\sigma$ are the mean and variance of $x$.Then we can say the variable follows the normal distribution.<br>If the variable $x$ follows the Gaussian distribution,we can denote this kind of distribution as:<blockquote><p>$x\sim N(\mu,\sigma^2)$</blockquote><h2 id=Multivariate-Normal-Distribution><a title="Multivariate Normal Distribution" class=headerlink href=#Multivariate-Normal-Distribution></a>Multivariate Normal Distribution</h2><p>For a mutivariable $n\times 1$ vector $X$ distributed following the multivariate distribution,with population mean vector $\mathbf{\mu}$ and a variance-covariance matrix $\Sigma$,the PDF(possibbility density function) can be described as:<blockquote><p>$\phi(X)=\frac{|\Sigma|^{\frac{-1}{2}}}{(2\pi)^{\frac{n}{2}}}\exp{\frac{-1}{2}(X-\mathbf \mu)^T\Sigma^{-1}(X-\mathbf\mu) }$</blockquote><p>If $n=2$ ,then we get the <em>bivariate normal distribution</em>,resulting in a bell-shaped curve in three dimensions.<h3 id=Mahalanobis-Distance><a title="Mahalanobis Distance" class=headerlink href=#Mahalanobis-Distance></a>Mahalanobis Distance</h3><p>Noticably,the mahalanobis distance is a distance!In the univariate case,if it is greater than zero,then the variable is larger than mean,vice versa.<br>In the multivariate case,since the covariance matrix $\Sigma$ is always positive definite,the distance is always greater than 0!<h4 id=In-Univariate-Normal-Distribution><a title="In Univariate Normal Distribution" class=headerlink href=#In-Univariate-Normal-Distribution></a>In Univariate Normal Distribution</h4><p>This distance can be measured as:<blockquote><p>$d=\frac{x-\mu}{\sigma}$</blockquote><h4 id=In-Multivariate-Normal-Distribution><a title="In Multivariate Normal Distribution" class=headerlink href=#In-Multivariate-Normal-Distribution></a>In Multivariate Normal Distribution</h4><p>Also,this distance can be measured as:<blockquote><p>$d=\sqrt{(X-\mu)^T\Sigma^{-1}(X-\mu)}$<h3 id=Integral-Property><a title="Integral Property" class=headerlink href=#Integral-Property></a>Integral Property</h3><p>For univariate normal distribution:<br>$\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}{\exp{\frac{-1}{2\sigma^2}(x-\mu)^2}}=1$</blockquote><p>For multivariate normal distribution:<blockquote><p>$\frac{1}{2\pi^{\frac{n}{2}}|\Sigma|^\frac{1}{2}}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}…\int_{-\infty}^{\infty}\exp{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)dx_1dx_2…dx_n}=1$</blockquote><h2 id=Diagonal-Covariance-Matrix-gt-Product-Gaussian><a title="Diagonal Covariance Matrix->Product Gaussian" class=headerlink href=#Diagonal-Covariance-Matrix-gt-Product-Gaussian></a>Diagonal Covariance Matrix->Product Gaussian</h2><p>Let’s imagine a diagonal covariance matrix $\Sigma$,OK,cool!This means the variables are independent!<br>Let’s take a $2\times 2$ covariance matrix for example:<br>In this column,we define $X=(x_1,x_2)$,the variance matrix $\Sigma = [\begin{matrix}<br>\sigma_1^2 & 0 \\\\<br>0 & \sigma_2^2<br>\end{matrix} ]$<br>and the mean vector $\mu=(\mu_1,\mu_2)$.<br>WE can easily compute the inverse matrix $\Sigma^{-1}=[\begin{matrix}<br>\frac{1}{\sigma_1^2} & 0 \\\\<br>0 & \frac{1}{\sigma_2^2}<br>\end{matrix} ]$<br>In this case,we can find the PDF as:<blockquote><p>$p(x;\mu,\Sigma)=\frac{1}{2\pi\sigma_1\sigma_2}\exp{(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))}$</blockquote><p>Further computing it,we get:<blockquote><p>$p(x;\mu,\Sigma)=\frac{1}{\sqrt{2\pi}\sigma_1}\exp{(-\frac{1}{2\sigma_1^2}(x_1-\mu_1)^2)}\frac{1}{\sqrt{2\pi}\sigma_2}\exp{(-\frac{1}{2\sigma_2^2}(x_2-\mu_2)^2)}$</blockquote><p>Further corollary shows for a n-dimensional variable vector $x$,we have:<br>$p(x;\mu,\Sigma)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma_{i}}\exp{(-\frac{1}{2\sigma_i^2}(x_i-\mu_i)^2)}$<h2 id=Ellipse-Where-Does-This-Thing-Come-From><a title="Ellipse:Where Does This Thing Come From??" class=headerlink href=#Ellipse-Where-Does-This-Thing-Come-From></a>Ellipse:Where Does This Thing Come From??</h2><h3 id=Isocontour-Hooray-Geography><a title="Isocontour(Hooray Geography!)" class=headerlink href=#Isocontour-Hooray-Geography></a>Isocontour(Hooray Geography!)</h3></div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/22/Variance-and-Covariance/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/22/Variance-and-Covariance/ itemprop=url>Variance and Covariance</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-22T14:38:20+08:00 title=创建于>2024-07-22</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-22T16:06:02+08:00 itemprop=dateModified title=更新于>2024-07-22</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/22/Variance-and-Covariance/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/22/Variance-and-Covariance/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>2 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=Variance><a class=headerlink href=#Variance title=Variance></a>Variance</h2><p>The <em>variance</em> of a random variable $X$ can be defined as:<blockquote><p>If $\mathbb E(X)=\mu$</blockquote><blockquote><p>$var(X)=\mathbb E((x-\mu)^2)$</blockquote><p>For its <em>standard deviation</em>,soometimes denotes as $sd(X)$,is defined as<blockquote><p>$sd(X)=\sqrt{var(X)}$</blockquote><p>The variance has the following properties:<blockquote><p>$var(X+C)=var(X)$<br>$var(bX)=b^2var(X)$</blockquote><p>Similarly,in the covariance matrix chapter,the <em>covariance matrix</em> has near-the-same properties.<h3 id=The-Tchebychev-inequality><a title="The Tchebychev inequality" class=headerlink href=#The-Tchebychev-inequality></a>The Tchebychev inequality</h3><blockquote><p>$\mathbb P{(|X-\mu|)>\epsilon}\leq \frac{var(X)}{\epsilon^2}$</blockquote><p>where $\mu$ is the expectation of $X$,and $\epsilon>0$<br>The variance can be sometimes canculated in another way:<blockquote><p>$var(X)=\mathbb E(X^2)-\mathbb E(X)^2$</blockquote><h2 id=Covariance><a class=headerlink href=#Covariance title=Covariance></a>Covariance</h2><p>The <em>covariance</em> of two random variables $X$ and $Y$ is defined as:<blockquote><p>$cov(X,Y)=\mathbb E{(X-\mathbb E(X))(Y-\mathbb E(Y))}$</blockquote><p>for a more complicated expression with $a,b,c,d$ as constants and $U,V,X,Y$ as random variables,we denote:<blockquote><p>$cov(aU+bV,cX+dY)=accov(U,X)+bccov(V,X)+adcov(U,Y)+bdcov(V,Y)$</blockquote><h2 id=Correlation><a class=headerlink href=#Correlation title=Correlation></a>Correlation</h2><p>The <em>correlation</em> between two variables $Y$ and $Z$ can be denoted as:<blockquote><p>corr(Y,Z)=\frac{cov(Y,Z)}{\sqrt{var(Y)var(Z)}}</blockquote><h2 id=Standardized-Variables><a title="Standardized Variables" class=headerlink href=#Standardized-Variables></a>Standardized Variables</h2><p>To standardize between different variables,we need to find a way:<em>standardized variables</em>.For two variables $Y$ and $Z$,with their mean and variance $\mu _Y$ $\sigma^2_Y$ $\mu _Z$ $\sigma^2_Z$.We standardize each with the following:<blockquote><p>$Y’=\frac{Y-\mu_Y}{\sigma_Y}$<br>$Z’=\frac{Z-\mu_Z}{\sigma_Z}$</blockquote><p>In this way,the mean of $Y’,Z’$ is 0 and the variance of them is 1,and<blockquote><p>$corr(Y,Z)=cov(Y’,Z’)=\mathbb E(Y’Z’)$</blockquote></div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/19/Analysis-How-to-Form-a-Paper/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/19/Analysis-How-to-Form-a-Paper/ itemprop=url>Analysis:How to Form a Paper</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-19T10:31:32+08:00 title=创建于>2024-07-19</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-22T15:16:33+08:00 itemprop=dateModified title=更新于>2024-07-22</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/19/Analysis-How-to-Form-a-Paper/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/19/Analysis-How-to-Form-a-Paper/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>1 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=Abstract><a class=headerlink href=#Abstract title=Abstract></a><strong>Abstract</strong></h2><p>$\quad$ Recent environment encourage students to create their own works or even present paper in the undergraduate period.But for most of the first and second year student,they find themselves hard to acquire the basic and needed skills and knowledge to structurize a formal paper with great proficiency.In order to prevent students from being tortured and to prevent peer viewers and mentors from relentlessly modifying and correcting paper,this work is proposed.The aim of this paper is to propose the conventional formula of how to c</div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/18/First-Glance-at-Covariance-Matrix/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/18/First-Glance-at-Covariance-Matrix/ itemprop=url>First Glance at Covariance Matrix</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-18T21:43:07+08:00 title=创建于>2024-07-18</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-19T08:49:12+08:00 itemprop=dateModified title=更新于>2024-07-19</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/18/First-Glance-at-Covariance-Matrix/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/18/First-Glance-at-Covariance-Matrix/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>3 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=Definition><a class=headerlink href=#Definition title=Definition></a>Definition</h2><p>Let $X$ be a $N\times 1$ random column vector.The <em>covariance matrix</em> of $X$ is denoted as $Var[X]$ ,and defined as<blockquote><p>$Var[X]=E[(X-E(X))(X-E[X])^T]$</blockquote><p>where $E(\cdot)$ denoted the expectation of $\cdot$.<br>In the equation above,we can transform the dot product form into square form,in other words:<blockquote><p>$E[(X-E(X))(X-E[X])^T]=E[(X-E(X))^2]$</blockquote><h2 id=Example><a class=headerlink href=#Example title=Example></a>Example</h2><p>Suppose $x$ is a $2\times 1$ vector,with $X_1,X_2$ as its components.<br>Let<blockquote><p>$Var[X_1]=2$<br>$Var[X_2]=4$<br>$Cov[X_1,X_2]=1$</blockquote><p>Noticably,<blockquote><p>$Var[X]=\begin{bmatrix}<br>Var[X_1] &Cov[X_1,X_2]\\<br>Cov[X_2,X_1] &Var[X_2]<br>\end{bmatrix}\<br>=\begin{bmatrix}<br>2 &1\\<br>1 &4<br>\end{bmatrix}<br>$</blockquote><p>The covariance matrix can also be counted by using the equivalent formula below:<blockquote><p>$Var[X]=E[XX^T]-E[X]E[X]^T$</blockquote><p>Also,we have such corollaries below:<blockquote><p>$Var[a+X]=Var[X]$</blockquote><blockquote><p>$Var[bX]=bVar[X]b^T$</blockquote><blockquote><p>$Var[X]^T=Var[X]$</blockquote><h2 id=Covariance-between-Linear-Transformations><a title="Covariance between Linear Transformations" class=headerlink href=#Covariance-between-Linear-Transformations></a>Covariance between Linear Transformations</h2><p>Let $a$ and $b$ be two constant $1\times K$ vectors and $X$ a $K\times 1$ random vector.The covariance between two linear transformations $aX$ and $bX$ is<blockquote><p>$Cov[aX,bX]=aVar[X]b^T$</blockquote><h2 id=Cross-Covariance><a class=headerlink href=#Cross-Covariance title=Cross-Covariance></a>Cross-Covariance</h2><p>For two random vectors,what will happen?<br>Let $X$ be a $K\times 1$ vector and $Y$ be a $L\times 1$ vector(random).The covariance matrix between $X,Y$ is denoted by $Cov[X,Y]$<blockquote><p>$Cov[X,Y]=E[(X-E(X)(Y-E(Y))^T]$</blockquote><p>resulting in a $K\times L$ matrix.<br>Apparently,$Cov[X,Y]\neq Cov[Y,X]$,but $Cov[X,Y]= (Cov[Y,X])^T$<br>What’s worth noting is that the Covariance Matrix bears good <em>linearity</em>!<h2 id=Reference><a class=headerlink href=#Reference title=Reference:></a>Reference:</h2><p><em>Taboga, Marco (2021). “Covariance matrix”, Lectures on probability theory and mathematical statistics. Kindle Direct Publishing. Online appendix. <a href=https://www.statlect.com/fundamentals-of-probability/covariance-matrix rel=noopener target=_blank>https://www.statlect.com/fundamentals-of-probability/covariance-matrix</a>.</em></div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/17/Reading-Pattern-Recognition-and-Machine-Learning/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/17/Reading-Pattern-Recognition-and-Machine-Learning/ itemprop=url>Reading:Pattern Recognition and Machine Learning</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-17T15:11:56+08:00 title=创建于>2024-07-17</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-17T17:25:17+08:00 itemprop=dateModified title=更新于>2024-07-17</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/17/Reading-Pattern-Recognition-and-Machine-Learning/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/17/Reading-Pattern-Recognition-and-Machine-Learning/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>2 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=Chapter-8-Graphical-Models><a title="Chapter 8:Graphical Models" class=headerlink href=#Chapter-8-Graphical-Models></a>Chapter 8:Graphical Models</h2><p>In graphical models,there are two parts:<blockquote><p>nodes or vertices<br>links or edges or arcs</blockquote><p>The graphical models mainly divide into two parts<blockquote><p><em>Bayesian networks</em> or <em>directed graphical models</em></blockquote><blockquote><p><em>Markov random fields</em> or <em>undirected graphical models</em></blockquote><blockquote><p>“Directed graphs are useful for expressing causal relationships between<br> random variables, whereas undirected graphs are better suited to expressing soft con<br>straints between random variables. For the purposes of solving inference problems,<br> it is often convenient to convert both directed and undirected graphs into a different<br> representation called a factor graph.”</blockquote><h3 id=Bayesian-Networks><a title="Bayesian Networks" class=headerlink href=#Bayesian-Networks></a>Bayesian Networks</h3><p><em>Multiplication Theorem of Probabilities</em><br>This can be denoted as:<blockquote><p>$p(a,b)=p(a|b)p(b)$</blockquote><p>which originates from the <em>conditional probability</em><blockquote><p>$p(A|B)=\frac{p(AB)}{p(B)}$</blockquote><p>For this relationship $P(a,b,c)$,we can take it separately as:<blockquote><p>$P(a,b,c)=P(c|a,b)P(a,b)=P(c|a,b)P(b|a)P(a)$</blockquote><p>This can be represented by a graph:<br><img title="Graph Denoting a,b,c" alt=图片 src=/images/graph81.png><br>Now let’s consider a more complicated scenario,there are N numbers in a column vector, polynomial coefficients $\mathbf{w}$.The input data $\mathbf{x}=(x_1,…,x_n)^T$ and the observed data$\mathbf{t}=(t_1,…,t_n)^T$,with a Gaussian noises variance $\sigma^2$ and a hyperparameter $\alpha$ representing the precision of the Gaussian prior over $\mathbf{w}$.<br>The joint distribution is given by the product of the prior $p(\mathbf{w})$ and $N$ conditional distributions $p(t_n|\mathbf{w})$ for $n=1,2,…,N$.<blockquote><p>$p(\mathbf{t,w})=p(\mathbf{w})\displaystyle \prod_{n=1}^{N}{p(t_n|\mathbf{w})}$</blockquote><h2 id=Appendix><a class=headerlink href=#Appendix title=Appendix></a>Appendix</h2><p>PDF: probability density function</div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/17/Reading-Paper-A-Low-Complexity-Massive-MIMO-Detection-Based-on-Approximate-Expectation-Propagation-AEP-or-EPA-in-this-paper/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/17/Reading-Paper-A-Low-Complexity-Massive-MIMO-Detection-Based-on-Approximate-Expectation-Propagation-AEP-or-EPA-in-this-paper/ itemprop=url>Reading Paper:A Low-Complexity Massive MIMO Detection Based on Approximate Expectation Propagation(AEP or EPA in this paper)</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-17T09:05:55+08:00 title=创建于>2024-07-17</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-22T15:16:33+08:00 itemprop=dateModified title=更新于>2024-07-22</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/17/Reading-Paper-A-Low-Complexity-Massive-MIMO-Detection-Based-on-Approximate-Expectation-Propagation-AEP-or-EPA-in-this-paper/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/17/Reading-Paper-A-Low-Complexity-Massive-MIMO-Detection-Based-on-Approximate-Expectation-Propagation-AEP-or-EPA-in-this-paper/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>5 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=Before-Beginning><a title="Before Beginning" class=headerlink href=#Before-Beginning></a>Before Beginning</h2><p>Now that we have some basic understanding of MIMO and relative mathematical knowledge,we will attempt to get the hang of this paper(whose knowledge still present a challenge to me)!<h3 id=A-Bit-of-Zero-Forcing-ZF><a title="A Bit of Zero Forcing(ZF)" class=headerlink href=#A-Bit-of-Zero-Forcing-ZF></a>A Bit of Zero Forcing(ZF)</h3><p>HFor a receiving antenna,if the channel matrix is $\mathbf{H}$,the sent message $\mathbf{s}$ and the received modulated message is $\mathbf{r}$ and denote the Gaussian noise as $\mathbf{n}$,then we denote the received message as:<blockquote><p>$\mathbf{r}=\mathbf{H\cdot s}+\mathbf{n}$</blockquote><p>The receiver don’t know about the sent messsage $\mathbf{s}$ so it decides to retrieve it from the received message $\mathbf{r}$.<br>Using MMSE,we can try to minimize$||\mathbf{r}-\mathbf{H\cdot s}||^2$<br>Obviously,we can get:<blockquote><p>$\hat{\mathbf{s}} = (\mathbf{H}^\mathbf{*})^{-1}\mathbf{H} \mathbf{r}$</blockquote><p>where the term before $\mathbf{r}$ is the pseudo-inverse of a matrix,$\mathbf{H^*}$ denotes the conjugate of $\mathbf{H}$.<br>The retrieved message,compared to the original $\mathbf{s}$,is:<blockquote><p>$\mathbf{\hat s}=\mathbf{s}+(\mathbf{n_m})$</blockquote><p>where $\mathbf{n_m}$ denotes the modified noise.<br>To eliminate the impact of possible influence of the modified noise,we add in a regularization factor $\delta \mathbf{I}$,so that:<blockquote><p>$$ \hat{\mathbf{s}} = (\mathbf{H}^\mathbf{*} + \delta \mathbf{I})^{-1} \mathbf{H} \mathbf{r} $$</blockquote><p>where:<br>if $\delta \to 0$,the channel matrix takes over so the $\hat {\mathbf s}$ will be just as the equation<br>if $\delta \to \infty$,the $\mathbf{I}$ takses over and in such cases,$\hat {\mathbf{s}}=\mathbf{H^*r}$<br>Completed!<h3 id=Neumann-Series><a title="Neumann Series" class=headerlink href=#Neumann-Series></a>Neumann Series</h3><h2 id=Introduction><a class=headerlink href=#Introduction title=Introduction></a>Introduction</h2><h3 id=Existing-Methods><a title="Existing Methods" class=headerlink href=#Existing-Methods></a>Existing Methods</h3><p>The loading factor $\alpha$ is defined as $\alpha=M/N$,with M transmitting and N receiving signals.<br>Existing MMSE and ZF methods exibit near -optimal performances when $\alpha <&LT1$,while carrying low spectral efficiency.<br>When $\alpha \approx 1$ the spectral efficiency can be attained while losing performance for linear detectors.<br>Later methods like BP and GTA still present not satisfying performance.<h3 id=Preliminary><a class=headerlink href=#Preliminary title=Preliminary></a>Preliminary</h3><h4 id=MIMO-System-Model><a title="MIMO System Model" class=headerlink href=#MIMO-System-Model></a>MIMO System Model</h4><p>In this paper,we use real-value system:<blockquote><p>$\mathbf{y}=\mathbf{Hx}+\mathbf{n}$</blockquote><p>where $\mathbf y\in \mathbb R^{2N}$,$\mathbf{H}\in \mathbb R^{2N\times 2M}$,$\mathbf{x}\in \Theta^{2M}$.$\mathbf n \sim \mathcal N(0,\sigma_n^2\mathbf I_{2N})$.<br>Assume a uniform prior distribution in this paper,meaning no tendency of a certain symbol.Expressed as $f(\mathbf x)\propto\prod_{i=1}^{2M}\mathbb I_{x_i\in \Theta}$,where $\mathbb I_{x_i\in \Theta}$ is the indicator duncition of the constraints of the value of $x_i\in \Theta$<br>If $x_i\in \Theta$,the function=1,else the function=0.<br>The posterior distribution of $\mathbf x$ has the following representation:<br>$p(\mathbf x|\mathbf y)\propto p(\mathbf y|\mathbf x)f(\mathbf x)\propto \mathcal N(\mathbf y :\mathbf{Hx},\sigma^2_n\mathbf I_{2N})\prod_{i=1}^{2M}{\mathbb I_{x_i\in \Theta}}$<h3 id=EP-MIMO-Detectors><a title="EP  MIMO Detectors" class=headerlink href=#EP-MIMO-Detectors></a>EP MIMO Detectors</h3><p>EP detector(EPD) enables a Gaussian approximation to be constructed for the posterior distribution of the transmitted symbols based on moment matching.<br>EP requires the explicit calculation of full matrix inversion,resulting in $\mathcal O(M^3)$ computation complexity.<br>The EP-NSA method exhibits great performance with $\alpha<&LT1$ but suffers performance degradation for larger $\alpha$.<br>The EP-SU method still costs around $\mathcal O(M^3)$.<br>The above methods involve circumventing <strong>matrix inversion</strong>!<br>Therefore,an EPA method is propopsed.Recovering EP performance for$\alpha&LT1$and approaches EP for $\alpha \geq 1$.<h3 id=Outline><a class=headerlink href=#Outline title=Outline></a>Outline</h3><p>The remainder of this paper is structured as below. The exact EP approach for massive MIMO detection is introduced in Section II. In Section III, detailed derivation, description and discussion of the proposed algorithm EPA are presented. Section IV shows an alternative derivation of EPA from the viewpoint of EC.Numericalsimulation results of the proposed algorithm and analysis of the computational costs are provided in Section V.Section VI concludes the paper.<h2 id=Simulation-Analysis><a title="Simulation Analysis" class=headerlink href=#Simulation-Analysis></a>Simulation Analysis</h2><p>Consider i.i.d.(independent and identically distributed)Rayleigh-fading channels with AWGN.<h2 id=Appendix><a class=headerlink href=#Appendix title=Appendix></a>Appendix</h2><p>ZF: zero forcing<br>CHEMP: channel-hardening exploiting message passing<br>GTA: Gaussian tree approximation<br>EP-NSA: EP with Neumann series approximation<br>EP-SU: EP-successive updating<br>EC: expectation consistency<br>i.i.d:independent and identically distributed<br>AWGN: additive white Gaussian noise</div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/15/compensation-KL-divergence/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/15/compensation-KL-divergence/ itemprop=url>compensation:KL divergence</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-15T16:20:26+08:00 title=创建于>2024-07-15</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-18T22:51:51+08:00 itemprop=dateModified title=更新于>2024-07-18</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/15/compensation-KL-divergence/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/15/compensation-KL-divergence/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>3 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=Reason><a class=headerlink href=#Reason title=Reason></a>Reason</h2><p>$\quad$In the Expectation Propagation(EP) algorithm,the KL (Kullback-Leibler) divergence plays an important role. Expectation Propagation is a technique used for approximate inference, commonly employed in probabilistic graphical models. In the Expectation Propagation algorithm, the KL divergence is used to measure the difference between two probability distributions in order to update variational parameters during the approximate inference process.<br>$\quad$Specifically, the Expectation Propagation algorithm approximates the <em>posterior</em> distribution by iteratively updating variational parameters. In each iteration, the KL divergence is used to quantify the <strong>discrepancy</strong> between the approximate posterior distribution and the true posterior distribution. By minimizing the KL divergence, the approximate posterior distribution can be brought closer to the true posterior distribution,enhancing the accuracy of the approximate inference.<h2 id=Definition><a class=headerlink href=#Definition title=Definition></a>Definition</h2><h3 id=What’s-Information-What’s-Entropy><a title="What’s Information?What’s Entropy?" class=headerlink href=#What’s-Information-What’s-Entropy></a>What’s Information?What’s Entropy?</h3><p>$\quad$We use information function $I(x)$ to measure the relationship between information and possibilities.For a certain event $x$,if the possibility that it happens is $P(x)$,then we define $I(x)$ as:<blockquote><p>$I(x)=-\log{(P(x))}$</blockquote><p>or<blockquote><p>$I(x)=\log{\frac{1}{(P(x))}}$</blockquote><p>Intuitively,the more possible,the less information.Match!<br>To sum up all information $I(x_i)$,we get the entropy!<blockquote><p>$H(X)=\displaystyle \sum_{i=1}^{n}{I(x_i)P(x_i)}$</blockquote><p>or<blockquote><p>$H(X)=\displaystyle -\sum_{i=1}^{n}{P(x_i)\log{(P(x_i))}}$</blockquote><p>where $X$ is a set with $n$ elements ${x_1,x_2,…,x_n}$,$x_i$ denotes a test.<h3 id=What’s-KL-Divergence><a title="What’s KL Divergence?" class=headerlink href=#What’s-KL-Divergence></a>What’s KL Divergence?</h3><p>KL(Kullback Leibler) divergence(or <em>relative entropy</em>) represents an asymmetric relationship between two probability distributions.<blockquote><p>$D_{KL}{(P||Q)}=\sum P(x)\log{}\frac{P(x)}{Q(x)}$ (in discrete scene)<br>$D_{KL}{(P||Q)}=\int P(x)\log{}\frac{P(x)}{Q(x)}dx$(in continuous scene)</blockquote><p>where P and Q are two distinct probability distributions on random variable $X$.<h3 id=What’s-Cross-Entropy><a title="What’s Cross Entropy?" class=headerlink href=#What’s-Cross-Entropy></a>What’s Cross Entropy?</h3><p>When we take a closer look at the KL divergence,we can break the $\log$ down into two pieces(take the discrete form as an example):<blockquote><p>$D_{KL}{(P||Q)}=\displaystyle \sum{P(x)\log(P(x))} -\sum{P(x)\log(Q(x))}$</blockquote><p>which can be further divided into:<blockquote><p>$-H(P(X))+[-\sum{P(x)\log{(Q(x))}}]$</blockquote><p>where we define the <em>cross entropy</em> of $P$ and $Q$ as:<blockquote><p>$H{(P,Q)}=\sum{-P(x)\log(Q(x))}$</blockquote><h2 id=Further-Application><a title="Further Application" class=headerlink href=#Further-Application></a>Further Application</h2><p><em><strong>To Be Continued</strong></em></div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/14/Reading-Paper-Belief-selective-Propagation-Detection-for-MIMO-systems/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/14/Reading-Paper-Belief-selective-Propagation-Detection-for-MIMO-systems/ itemprop=url>Reading Paper:Belief-selective Propagation Detection for MIMO Systems</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-14T21:50:08+08:00 title=创建于>2024-07-14</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-18T15:20:47+08:00 itemprop=dateModified title=更新于>2024-07-18</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/14/Reading-Paper-Belief-selective-Propagation-Detection-for-MIMO-systems/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/14/Reading-Paper-Belief-selective-Propagation-Detection-for-MIMO-systems/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>12 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=Prerequisites><a class=headerlink href=#Prerequisites title=Prerequisites></a>Prerequisites</h2><p>$\quad$For a first year student who have been arranged to not take courses like <em>probability theory</em>,the famed Bayesian things are not familiar to me at all.So I prepare some prerequisite knowledge here!<h3 id=Bayes’-theorem><a title="Bayes’ theorem" class=headerlink href=#Bayes’-theorem></a>Bayes’ theorem</h3><p>$\quad$In this theorem,we define two events $A$ and $B$,each having $P(A),P(B)$ to denote their possibilities of happening.The Bayes’ theorem considers the influence on the possibilities of each other.We denote event that if B happens,A also happens using $A|B$.<br>The Bayes’ theorem tells us that<blockquote><p>$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$</blockquote><p>we give each possibility in this formula<br>a name:<blockquote><p>$P(A)$ is $A$’s prior probability(which means it does not take any related event into account,only considering the general case)<br>$P(A|B)$ is $A$’s posterior probability(which means that after B happens,the possibility that A happens should be refreshed)</blockquote><h3 id=Maximum-Likelihood-Estimate-MLE><a title="Maximum Likelihood Estimate(MLE)" class=headerlink href=#Maximum-Likelihood-Estimate-MLE></a>Maximum Likelihood Estimate(MLE)</h3><p>$\quad$For a random test(like flipping coins)<br>define the possibility of event <em>Heads</em> $\theta$,then event <em>Tails</em> is defined as $(1-\theta)$.in one test,<em>Heads</em> happen $a$ times and <em>Tail</em> happen $b$ times.We define the possibilty that this test happens as $N$.Define the possibility $P(N|\theta)$as the <em>likelihood function</em> $L(\theta)$:<blockquote><p>$L(\theta)=\theta^a*(1-\theta)^b$</blockquote><p>where we need to find the most possible point to let the function have its extreme point.Take the logarithm on both sides of the function equation:<blockquote><p>$\log{(L(\theta))}=a\log{(\theta)}+b\log{(1-\theta)}$</blockquote><p>take the derivative of it,we can easily get the extreme point!<h3 id=Maximum-a-Posterioris-Estimate-MAPE><a title="Maximum a Posterioris Estimate(MAPE)" class=headerlink href=#Maximum-a-Posterioris-Estimate-MAPE></a>Maximum <em>a Posterioris</em> Estimate(MAPE)</h3><p>$\quad$With the two parts of knowledge in mind,we are well ready to get our hands on Maximum <em>a posterioris</em>!<br>$\quad$As its name suggests,<em>a posterioris</em> means after.Then estimating based on prior tests is the core of this method.<br>$\quad$We define a set of tests $S$,and the desired possibility of the event $\theta$ is happening is defined as<blockquote><p>$P(\theta|S)$</blockquote><p>$\quad$According to the Bayes’ theorem,the possibility above can be described as<blockquote><p>$P(\theta|S)=\frac{P(S|\theta)P(\theta)}{P(S)}$</blockquote><p>$\quad$Obviously,the $P(S|\theta)$ refers to the <em>most likelihood</em> function $L(\theta)$.Easily,we can calculate the MAP possibility.<h2 id=Abstract><a class=headerlink href=#Abstract title=Abstract></a>Abstract</h2><p>simplifying calculations will cost performance quality?<br>utilizing <em>trusted</em> incoming messages with <em>a priori</em> messages for updates.<br>proposed two strategies:<blockquote><p>symbol-based truncation(ST)(基于特征的去杂)<br>edge-based simplification(ES)(基于边缘的简化)<br>conveniently tuning parameters can make great trade-off between performance and complexity</blockquote><h2 id=Introduction><a class=headerlink href=#Introduction title=Introduction></a>Introduction</h2><p>$\quad$MIMO enables high spectrum-efficiency and energy-efficiency.<br>$\quad$Current <em>maximum a posteriori</em> (MAP) detection(最大后验检测) or the <em>maximum likelihood</em> detection(极大似然检测) can achieve minimum error probability while costing exponential complexity with modulation order and MIMO scale.<br>$\quad$Sphere decoder(SD) restricts candidate symbols to ones within a sphere (depth first) or a list of size $K$ (width first).It performs well in small-size MIMO, while for large-scale MIMO it suffers cubic($\mathcal{O}^3$) time complexity.<br>$\quad$For large-scale MIMO,linear detectors like <em>zero-forcing</em>(ZF) detector or <em>linear minimum mean square error</em>(LMMSE) detector are often applied,performing way worse than the previous methods.<br>$\quad$Another problem is the matrix inversion,bringing excessive complexity.Even new methods like the <em>Neumann series approximation</em> can’t fully exploit the MIMO benefits.<br>$\quad$<em>Finally</em>,the Belief Propagation(BP)detector based on the Bayes’ tule is proposed.It can delever near-optimal performance and holds a soft-input soft-output character.And it can be easily applied to various hardware and scaled for applications.<br>$\quad$The BP method achieves near-optimal when in small-to medium MIMO but costs exponentially increasing complexity.<br>$\quad$The work in this paper focuses on both reducing the complexity and maintaining performance.<!--So far, the design of efficient MIMO BP detectors remains challenging. First, the existing
 BP detectors suffer an aggravated performance error floor in relatively high signal-to-noise ratio
 (SNR) region, due in part to the well-known impact of the inherent loopy structure for the full
connected FG model [23], in part to the approximations such as GAI. Second, the complexity of
 existing BP detectors is still high and not flexible enough for various applications. These multiple
 challenges motivate us(us?) to think of an approach which can reduce the detection complexity while
 mitigating the error floor towards better performance.--><p>$\quad$This aim is mainly achieved by introducing the Belief-selective Propagation(BsP),utilizing <em>a priori</em> (AP) probabilities to update incoming messages to output messages.This approach avoids propagating “low belief” messages.<h2 id=Preliminaries><a class=headerlink href=#Preliminaries title=Preliminaries></a>Preliminaries</h2><p>$N_t,N_r$ denote FN number and SN number(implying $N_r$ messages).<h3 id=System-Model><a title="System Model" class=headerlink href=#System-Model></a>System Model</h3><p>QAM:Quadrature Amplitude Modulation<br>$s=[s_1,s_2,…,s_N{_t}]^T$<br>The flat-fading complex MIMO channel matrix $\mathbf {H}$ is a $N_r\times N_t$ matrix:<blockquote><p>$\mathbf {H}=[\mathbf {h_1}^{T},\mathbf {h_2}^{T},…\mathbf {h_{N_r}^{T}}]^T$</blockquote><p>where each component $\mathbf {h_i}$ is a complex channel coefficient.<br><!--is a complex channel coefficient following the zero-mean and unit-variance Gaussian distribution
 Why this is less unlikely to happen in the future communication?--><h3 id=BP-Detection-With-the-FG-Model><a title="BP Detection With the FG Model" class=headerlink href=#BP-Detection-With-the-FG-Model></a>BP Detection With the FG Model</h3><p>$\quad$In a <em>factor graph</em>(FG) model,there are two types of nodes,namely <em>factor node</em>(FN) and <em>symbol node</em>(SN),denoted in graphs as:<br>$\begin{cases}f_i \Leftrightarrow FN, & i\in {1,2,…,N_r} \ S_j\Leftrightarrow SN, & j\in {1,2,…,N_t}\end{cases}$<br>$\quad$In both the factor graph and the tanner graph,we use the channel matrix $\mathbf {H}$ to denote elements. To be exact,each $f_i$ corresponds to the $i$-th row of the channel matrix,and each $S_j$ relates to the $j$-th column of the channel matrix. Every pair<br>of $f_i$ and $S_j$ are connected with an edge corresponding to the channel coefficient $h_{ij}$.<br>$\quad$In this article ,message $\beta_{ij}$ will be delivered from $f_i$ to $S_j$,while the message $\alpha$ will be computed and transferred in a reverse direction,meaning $\alpha_{ji}$ will be message sent from $S_j$ to $f_i$.<h2 id=The-Proposed-BsP-Detection><a title="The Proposed BsP Detection" class=headerlink href=#The-Proposed-BsP-Detection></a>The Proposed BsP Detection</h2><p>$\quad$The constellation cardinality($|(\mathcal A)|$) of the MIMO systems and the degree<!--?--> of the function nodes(FNs) contribute greatly to the exponential computational complexity.<br>$\quad$However symbol vectors with low reliability contribute limitly to the message $\beta_{ij}$,encouraging us to squeeze the search space by removing them.<!-- 1)STstrategy to reduce thecardinalityof the
 transmittedsymbol
  2)ESstrategy tosimplified theconnected
 edgesof theFNs--><p><strong>$\quad$The two strategies can be concluded in a simple metaphor.Here comes a bunch of people $\alpha_{ji}$,with $j\in \mathbf{k}$,each with a number of messages.Based on the credibility of each person,you select $d_f$ most credible people to receive message,being the ES strategy.For each independent person, you select $d_m$ most credible messages to hear,being the ST strategy.</strong><h3 id=Symbol-Based-Truncation-Strategy><a title="Symbol-Based Truncation Strategy" class=headerlink href=#Symbol-Based-Truncation-Strategy></a>Symbol-Based Truncation Strategy</h3><p><strong>Here,this ST strategy can be simply described in a sentence:Man</strong><br>$\quad$Incoming $\alpha_{ji}{}$ is truncated into $\alpha_{ji}^{t}$ by eliminating the <em>log-likelihood ratio</em>(LLR) with relatively small value.<br>$\quad$Formula unheard of to compute message $\beta_{ij}$.With $\mu_k$ and $\mu_1$,I think this in part looks like the <strong>LLR</strong>?<br>$\quad$Core is to eliminate low credibility incoming data $\alpha$ into $\alpha^t$(truncated).This procedure is basesd on the LLR.<br>$\quad$In this paper,this method(ST) successfully made it to reduce the number of possible choices of transmitted symbols to:<blockquote><p>$\psi_{ST}=|\mathcal A|\times |\mathcal B(d_m)|$</blockquote><p>where $\mathcal B(d_m)$ denotes the configuration set of possible choices of $\mathbf{s_k}_{\backslash j}$,with $|\mathcal B(d_m)|=(d_m)^{(N_t-1)}$<br>Because $d_m << |\mathcal A|$,the ST strategy reduces the computaional complexity.<h3 id=Edge-Based-Simplification-Strategy><a title="Edge-Based Simplification Strategy" class=headerlink href=#Edge-Based-Simplification-Strategy></a>Edge-Based Simplification Strategy</h3><p>Apart from the ST strategy,the ES strategy further reduces the computational complexity.<br>The core if it is to directly decide some of the transmitted symbol $\mathbf {s_{k}}_{\backslash j}$.<br>By choosing the most reliable message (judging by LLR),we can denote the new configuration set as$\mathcal B(|\mathcal A|,d_f)$:<blockquote><p>$\mathcal B(|\mathcal A|,d_f)={ {\mathbf{s_k}_ {\backslash j} }=[\mathbf { {s_k} }<em>{\backslash j,d_f};\mathbf {\bar{s_k}</em>{\backslash j,d_f}^T]} }$</blockquote><p>In this sense,the cardinality of $\mathcal B(|\mathcal A|,d_f)$ is<blockquote><p>$|\mathcal B(|\mathcal A|,d_f)|=\binom{N_t-1} {d_f-1}|\mathcal A|^{d_f-1}$</blockquote><p>and the number of possible choices are reduced by the ES strategy to:<blockquote><p>$\psi_{ES}=|\mathcal A|\times \binom{N_t-1} {d_f-1}|\mathcal A|^{d_f-1}$</blockquote><h3 id=Initialization-with-PP-Information><a title="Initialization with PP Information" class=headerlink href=#Initialization-with-PP-Information></a>Initialization with PP Information</h3><p>Core algorithm:LMMSE(Linear Minimum Mean Square Estimate).<blockquote><p>$\hat{\mathbf s}_{\text{MMSE} }=(\mathbf{H}^H\mathbf H+\sigma^2\mathbf{I})^{-1}\mathbf{H}^H\mathbf{y}$.</blockquote><p>Where we denote:<blockquote><p>$$\mathbf{K}=(\mathbf H^H\mathbf H+\sigma^2\mathbf{I})^{-1}$$</blockquote><p>With such,<em>a priori</em> probabilities of transmitted symbols are computed by<blockquote><p>$p_j(\mu_k) = \exp {-\frac{||\mu_k - \hat{\mathbf{s}}{\text{MMSE}}||^2}{2\sigma{\text{MMSE}_j}^2} }$</blockquote><h3 id=The-Proposed-BsP-Detector><a title="The Proposed BsP Detector" class=headerlink href=#The-Proposed-BsP-Detector></a>The Proposed BsP Detector</h3><h4 id=Algorithm-of-the-BsP-Detection><a title="Algorithm of the BsP Detection" class=headerlink href=#Algorithm-of-the-BsP-Detection></a>Algorithm of the BsP Detection</h4><p>This is a rather interesting algorithm!<br>Input:received signal $\mathbf y$ and channel matrix $\mathbf H$<br>Output:soft information of coded bits $\mathbf r$<h2 id=Appendix><a class=headerlink href=#Appendix title=Appendix></a>Appendix</h2><p>PP: pseudo priori<br>(|$\mathcal A$|): constellation cardinality<br>LLR: log-likelihood ratio<br>AMI: average mutual information<br>BER: bit error rate or block error rate(误码率和误信率)</div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/14/Reading-Paper-factor-graphs-and-the-sum-product-algorithm/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/14/Reading-Paper-factor-graphs-and-the-sum-product-algorithm/ itemprop=url>Reading Paper:factor graphs and the sum product algorithm</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-14T11:54:48+08:00 title=创建于>2024-07-14</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-18T22:53:06+08:00 itemprop=dateModified title=更新于>2024-07-18</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/14/Reading-Paper-factor-graphs-and-the-sum-product-algorithm/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/14/Reading-Paper-factor-graphs-and-the-sum-product-algorithm/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>8 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=Factor-Graph-因子图><a title="Factor Graph(因子图)" class=headerlink href=#Factor-Graph-因子图></a>Factor Graph(因子图)</h2><h3 id=composition><a class=headerlink href=#composition title=composition></a>composition</h3><p>Factor graphs are composed of two parts:<em>(random)variables and factors</em><br>e.g. a funtion imply(A,B) has the meaning of if A,then B<br>here represents two variables: A:Dave is drunk B:Dave can’t drive<br>as is obvious,if a person is drunk,then he(she) can’t drive<br>hence a simple$f(drunk,can’t\space drive)$represents the meaning.<h3 id=weight-of-functions–probability><a title="weight of functions–probability" class=headerlink href=#weight-of-functions–probability></a>weight of functions–probability</h3><p><em>To define the probability of a possible world,we define through factor functions</em><br><strong>weights</strong>are given to different factor funtions,to express the relative influence of each factor on the probability.Factors can be given larger weight to imply a higher impact on probability.<blockquote><p><strong>Amazingly,when I refer to a post at <a href=http://deepdive.stanford.edu/assets/factor_graph.pdf rel=noopener target=_blank>http://deepdive.stanford.edu/assets/factor_graph.pdf</a> to look up for definitions,the post contains the link of the <em>Factor Graphs and the Sum-Product Algorithm</em> ,which really surprises me</strong></blockquote><blockquote><p>the <em>Factor Graphs and the Sum-Product Algorithm</em> mentioned the factor graph has the potential to unify modeling and signal processing tasks that are often treated separately in current systems,then if now,some 20 years have passed,any development based on it?</blockquote><h3 id=history–why-factor-graph><a title="history–why factor graph" class=headerlink href=#history–why-factor-graph></a>history–<em>why factor graph</em></h3><p>factor graphs are a generalization of the “Tanner graphs”named after Tanner.He introduced the bipartite graphs(dividing group of nodes into two parts connected by edges)to generalize family of codes performing low-density-parity-check(LDPC) and described the sum-product algorithm.Tanner and Wiberg respectively introduced the “visible” and “hidden”variables.However,by applying these models to functions,the factor graph becomes a higher abstraction.Viewing from the perspective of factor graph,the Tanner graph represents a particular factorization of the code.<h4 id=marginal-product-of-functions-not-in-this-algorithm-unfortunately><a title="marginal product of functions(not in this algorithm,unfortunately)" class=headerlink href=#marginal-product-of-functions-not-in-this-algorithm-unfortunately></a><em>marginal product of functions</em>(not in this algorithm,unfortunately)</h4><p>The marginal product stems from the total product,which describes that for a set of variables(e.g.$x_1,x_2,x_3$),the overall output is $F(x_1,x_2,x_3)$.Set the other inputs fixed(say fix $x_2,x_3$ are fixed to $a,b$) and simply vary one input(say vary $x_1$),then the output becomes $F(x_1,a,b)$,which,in other words,can be described as the total product function $TP(x_1)$.<br>The marginal product function describes the slope of the total product funtion.If the total product function is differentiable,then the marginal product can be described as$MP(x_1)=\frac{dTP(x_1)}{dx_1}$<br>btw,the average product of a function is described as $AP(x_1)=\frac{TP(x_1)}{x_1}$<h2 id=Marginal-Functions-Factor-Graphs-and-the-Sum-Product-Algorithm><a title="Marginal Functions,Factor Graphs and the Sum-Product Algorithm" class=headerlink href=#Marginal-Functions-Factor-Graphs-and-the-Sum-Product-Algorithm></a>Marginal Functions,Factor Graphs and the Sum-Product Algorithm</h2><p>Let n variables be $x_1,x_2,…,x_n$,each $x_i \in A_i$.<br>Let $g(x_1,…,x_n)$be an R-valued function of variables $x_1,x_2,,…,x_n$<br>so that the function $g$ is with domain $S=A_1 \times A_2 \times … \times A_n$ and codomain $R$.<br>$S$ is called the <em>configuration space</em>for the given variables.Each element is a particular <em>configuration</em> of the variables<br>associated with every function $g(x1,…,x_n)$are $n$ marginal functions $g_i(x_i)$<br><strong>And for each $a\in A_i$,the value of $g_i(a)$ is obtained by summing the value of $g(x_1,…,x_n)$over all configurations of the variables that have $x_i=a$</strong><br>The paper proposes a nontraditional notation–the”not-sum” or <em>summary</em>.For example,if $g$ is a function of variables$x_1,x_2,x_3$,then the <em>summary</em> for $x_2$ can be denoted by:<blockquote><p>$g_2(x_2)=\sum_{\sim{x_2}}{g(x_1,x_2,x_3)}=\sum_{x_1\in A_1}{\sum_{x_3\in A_3}{g(x_1,x_2,x_3)}}$</blockquote><p>in this sense:<blockquote><p>$g_i(x_i)=\sum_{\sim{x_i}}{g(x_1,…,x_n)}$</blockquote><p>that means the $i$th marginal function associated with $g$ is the <em>summary</em> for $x_i$ of $g$<br>In this paper,another topic is about transforming global functions into the product of several global functions.<br>Let $g(x_1,…,x_n)$ factor into the product of several <em>local functions</em> $f_i$,each having some subset of ${x_1,…,x_n}$ as arguments (e.g. $J={1,2,3,4,5}, X_1={x_1,x_3,x_5},X_2={x_2,x_3}$).<blockquote><p>$g(x_1,…,x_n)=\prod_{j\in J}{f_j(X_j)}$</blockquote><p>Followed by several examples of factor graphs<br>A key is that <em><strong>a cycle-free factor graph not only encodes in its structure but also encodes arithmetic expressions by which the marginal functions associated with the global functions may be computed.</strong></em><h2 id=Core-of-the-algorithm><a title="Core of the algorithm" class=headerlink href=#Core-of-the-algorithm></a>Core of the algorithm</h2><h3 id=Sum-And-Product><a title="Sum And Product" class=headerlink href=#Sum-And-Product></a>Sum And Product</h3><p>As its name suggests,the algorithm can be derived into two parts:the sum part and the product part,which cna be transformed into each other with ease.Noticably,the graphical demonstration of it involves the factor graph.<h3 id=Converted-factor-graph–expression-trees><a title="Converted factor graph–expression trees" class=headerlink href=#Converted-factor-graph–expression-trees></a>Converted factor graph–expression trees</h3><p>expression trees are a kind of tree that contain arithmetic operators($+,\times$…etc.)in internal vertices and variables or constants in leaf vertices.<br>An extended version of the expression tree is proposed,using <em>functions</em> besides variables and constants.<br>replace variable node with a product operator.<br>replace factor node with a “form product” and multiply by”$f$” operator.<blockquote><p>my own interpretation:<br><em>both</em><br><strong>bottom-up approach</strong><br>$f$ is the local function that operates<br>$x$ works as variables</blockquote><blockquote><p><em>In expression tree</em><br>the tree itself expresses the algorithm<br>$+ \text{,and}\times$ work as operators to combine these nodes together</blockquote><blockquote><p><em>In rooted tree</em><br>expresses the process and relationship between nodes<br>factor nodes and variable nodes come in an alternating form.</blockquote><h3 id=Sum-Product-Updating-Rule><a title="Sum Product Updating Rule" class=headerlink href=#Sum-Product-Updating-Rule></a>Sum Product Updating Rule</h3><h4 id=update><a class=headerlink href=#update title=update></a>update</h4><p>a node receives all messages from nodes adjacent to it except for the node to send message<br>In this approach,all adjacent nodes of node $v$ are denoted as $n(v)$.<br>Let the message be $\mu_{x\to f}{(x)}$ ,denoting message sent from node $f$ to node $x$,and $\mu_{f\to x}{(x)}$ be message sent from node $x$ to node $f$.<br>The rules are as follows:<br><em>variable to local function</em><br>$\mu_{x\to f}{(x)}=\prod_{h\in n(x) \backslash {f}}{\mu_{h\to x}{(x)}}$<br>simply speaking:to operate product on all messages from other nodes and send them to the descending factor node $f$.<br><em>local function to variable</em><br>$\mu_{f\to x}{(x)}=\sum_{\sim{x}}{(f(X)\prod_{y\in n(f)\backslash{x}}{\mu_{y\to f}{(y)}})}$<br>simply put:to operate product of all messages received and times the local function,sum up the results with index $\sim{x}$<br>$X=n(f)$ is the set of arguments of $f$<h4 id=end-termination><a class=headerlink href=#end-termination title=end(termination)></a>end(termination)</h4><p>Representing the arithmetic form with factor graph,we can easily see that for a variable node $x_i$,its”summary”$g_i(x_i)$ can be described as the product of all messages sent from all edges to the specific node.<br>For example,if a variable node $x_1$ is connected to factor nodes $f_1$ and $f_3$,then the marginal function of $x_1$ can be denoted as<blockquote><p>$g_1(x_1)=\mu_{f_1\to x_1}{(x_1)}\mu_{f_3\to x_1}{(x_1)}$</blockquote></div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/12/%E6%A8%A1%E7%94%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/12/%E6%A8%A1%E7%94%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/ itemprop=url>模电学习笔记1</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-12T15:35:35+08:00 title=创建于>2024-07-12</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-12T17:51:48+08:00 itemprop=dateModified title=更新于>2024-07-12</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/12/%E6%A8%A1%E7%94%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/12/模电学习笔记1/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>4 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h1 id=本征半导体和杂质半导体><a class=headerlink href=#本征半导体和杂质半导体 title=本征半导体和杂质半导体></a>本征半导体和杂质半导体</h1><h2 id=半导体><a class=headerlink href=#半导体 title=半导体></a>半导体</h2><p>顾名思义，半导体就是指导电能力处于绝缘体和导体之间的物质，如硅，锗等。<h2 id=本征半导体><a class=headerlink href=#本征半导体 title=本征半导体></a>本征半导体</h2><h3 id=本征半导体的定义><a class=headerlink href=#本征半导体的定义 title=本征半导体的定义></a>本征半导体的定义</h3><p>本征半导体就是纯净的半导体，具有晶体结构。一般是四价元素，彼此靠共价键链接。<br>在绝对零度时，其中的价电子被束缚住，没有导电性能。<br>温度升高到常温下（300K）时，一些价电子得以脱离束缚，成为自由电子，相应的共价键上留下一个空位，称为空穴<h3 id=本征半导体的载流子><a class=headerlink href=#本征半导体的载流子 title=本征半导体的载流子></a>本征半导体的载流子</h3><p>由定义可以发现，本征半导体的载流子分为两种：<br>1.自由电子(free electron)<br>2.空穴(mobile hole)<h3 id=本征半导体的载流子浓度><a class=headerlink href=#本征半导体的载流子浓度 title=本征半导体的载流子浓度></a>本征半导体的载流子浓度</h3><p>公式：<blockquote><p>$n_i=p_i=K_1T^{3/2}e^{-\frac{E_G}{2kT}}$</blockquote><p>其中：<blockquote><p>$n_i:\text{自由电子的浓度}$ $p_i:\text{空穴的浓度}$<br>$K_1:\text{系数（与材料有关）}$<br>$T:\text{绝对温度}$<br>$k:\text{玻尔兹曼常数}$<br>$E_G:\text{禁带宽度（价电子挣脱共价键所需能量）}$</blockquote><h3 id=本征半导体的导电能力><a class=headerlink href=#本征半导体的导电能力 title=本征半导体的导电能力></a>本征半导体的导电能力</h3><p>有极限！半导体的导电能力有极限，如何改进？<h2 id=杂质半导体><a class=headerlink href=#杂质半导体 title=杂质半导体></a>杂质半导体</h2><h3 id=杂质半导体的定义><a class=headerlink href=#杂质半导体的定义 title=杂质半导体的定义></a>杂质半导体的定义</h3><p>注意到，半导体具有扩散特性，可将少量杂质元素掺入其中<br>如正四价元素中掺入五价（或者三价）元素，将少部分正四价元素取代<br>如正五价元素掺入之后，会出现很多自由电子<br>导电能力提高上百万倍！<h3 id=N型半导体><a class=headerlink href=#N型半导体 title=N型半导体></a>N型半导体</h3><p>形成：掺入P（磷元素）<br>可出现两种载流子：自由电子（多数），空穴（少数）<br>因而自由电子是多子（多数载流子），空穴是少子（少数载流子）<br>由于带负电的自由电子占多数，其被称为 <em><strong>N（negative）型半导体</strong></em><br><strong>需要注意，每多一个自由电子，就会出现一个磷离子，因而整体依然呈现电中性</strong><br>温度敏感：由于温度影响的是本征激发，所以其对整体导电能力影响小，但是对少数载流子的影响依然大<h3 id=P型半导体><a class=headerlink href=#P型半导体 title=P型半导体></a>P型半导体</h3><h4 id=P有个洞！是空穴！所以多数载流子是空穴！其他同N型半导体><a class=headerlink href=#P有个洞！是空穴！所以多数载流子是空穴！其他同N型半导体 title=P有个洞！是空穴！所以多数载流子是空穴！其他同N型半导体></a>P有个洞！是空穴！所以多数载流子是空穴！其他同N型半导体</h4><h1 id=PN结><a class=headerlink href=#PN结 title=PN结></a>PN结</h1><h2 id=PN结的定义><a class=headerlink href=#PN结的定义 title=PN结的定义></a>PN结的定义</h2><p>顾名思义，PN结合在一起，就是PN结（在P，N型半导体的交界面形成），PN结具有单向导电性（可用于制作二极管）<h2 id=PN结的形成><a class=headerlink href=#PN结的形成 title=PN结的形成></a>PN结的形成</h2><blockquote><p><img alt=图片 src=/images/PNJunction1.jpg title=PN结示意图（1）></blockquote><p>在PN半导体的交界面处，由于两种半导体中的载流子浓度差距很大，发生<strong>扩散运动</strong>，P区的空穴向N区扩散，N区的自由电子向P区扩散，如 <em>(a)<em>所示<br>在这其中P区出现负离子区，N区出现正离子区，这两个区域合起来叫做<strong>空间电荷区</strong>（或者<strong>耗尽层</strong>），形成电场。扩散运动强的时候（载流子浓度强），空间电荷区宽，其内部的电场强度加强。电场强度方向由N区指向P区，阻止了扩散运动的进行，在这之中P区的自由电子向N区流动，N区的空穴向P区流动（注意都是少子），这种运动就称为**漂移运动</em></em><br>这其中，当参与扩散运动的多子和参与漂移运动的少子数量达到动态平衡的时候，PN结形成。<h3 id=对称-不对称PN结><a class=headerlink href=#对称-不对称PN结 title=对称/不对称PN结></a>对称/不对称PN结</h3><p>P区与N区杂质浓度相等时，负离子区和正离子区的宽度也相等，称为<strong>对称结</strong><br>浓度不相等时，浓度 <em>高</em>一侧的离子区宽度 <em>低</em>于浓度低一侧的离子区宽度，称为<strong>不对称PN结</strong><blockquote><p>理解：浓度高一侧的载流子扩散运动较强，将浓度低一侧的离子区拉宽</blockquote><p>两种PN结的外部特性相同<h2 id=PN结的单向导电性><a class=headerlink href=#PN结的单向导电性 title=PN结的单向导电性></a>PN结的单向导电性</h2><h3 id=正向><a class=headerlink href=#正向 title=正向></a>正向</h3><blockquote><p><img alt=图片 src=/images/PNJunction2.jpg title=PN结示意图（2）></blockquote><p>P接正极，N接负极时，称为PN结外加<strong>正向电压</strong>，外电场削弱内电场，使得扩散运动加剧，漂移运动减弱。显然，在外接正向电压达到一个临界数值之前，其电流不会有很大变化，但是一旦达到临界，电流就会急剧增大，形成<strong>正向电流</strong>，PN结<strong>导通</strong>，此时PN结的压降只有0.几V，因此需要串联限流电阻防止元器件损坏<blockquote><p>郑益慧老师：电源和二极管总要烧一个</blockquote><h3 id=反向><a class=headerlink href=#反向 title=反向></a>反向</h3><blockquote><p><img alt=图片 src=/images/PNJunction3.jpg title=PN结示意图（3）></blockquote><p>加<strong>反向电压</strong>时，内电场被加强，空间电荷区（耗尽层）加宽，加剧了漂移运动，抑制了扩散运动，形成<strong>反向电流</strong>，由于漂移运动是少子在进行的，其数目极少，导致反向电流极小，一般可以忽略，认为其在加反向电压时处于<strong>截止</strong>状态。<h2 id=PN结的电流方程><a class=headerlink href=#PN结的电流方程 title=PN结的电流方程></a>PN结的电流方程</h2><p>PN结所加端电压$u$与其电流$i$的关系为<blockquote><p>$i=I_s(e^{\frac{u}{U_T}}-1)$</blockquote><p>常温下（$T=300K$）时，$U_T\approx26mV$<br>称$U_T$为温度的电压当量</div><footer class=post-footer><div class=post-eof></div></footer></div></article></section><nav class=pagination><span class="page-number current">1</span><a class=page-number href=/page/2/>2</a><a class="extend next" href=/page/2/ rel=next>></a></nav></div></div><div class=sidebar-toggle><div class=sidebar-toggle-line-wrap><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside class=sidebar id=sidebar><div class=sidebar-inner><section class="site-overview-wrap sidebar-panel sidebar-panel-active"><div class=site-overview><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt class=site-author-image itemprop=image src=/images/avatar.png><p class=site-author-name itemprop=name><p class="site-description motion-element" itemprop=description></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>12</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-tags"><a href=/tags/index.html> <span class=site-state-item-count>2</span> <span class=site-state-item-name>标签</span> </a></div></nav><div class="links-of-author motion-element"><span class=links-of-author-item> <a href=https://github.com/JohnnyZhu035 target=_blank title=GitHub> <i class="fa fa-fw fa-github"></i></a> </span><span class=links-of-author-item> <a href=213232918@seu.edu.cn target=_blank title=E-Mail> <i class="fa fa-fw fa-envelope"></i></a> </span></div></div></section><div class=back-to-top><i class="fa fa-arrow-up"></i><span id=scrollpercent><span>0</span>%</span></div></div></aside></div></main><footer class=footer id=footer><div class=footer-inner><div class=copyright>© <span itemprop=copyrightYear>2024</span><span class=with-love> <i class="fa fa-user"></i> </span><span class=author itemprop=copyrightHolder>Johnny</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-area-chart"></i> </span><span title=站点总字数>NaNm</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点总阅读次数>NaN:aN</span></div><div class=theme-info>主题 — <a class=theme-link href=https://github.com/theme-next/hexo-theme-next target=_blank>NexT.Pisces</a> v6.0.0</div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("7/10/2024 0:00:00"); //修改为你的网站开始运行的时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "这个网站已经上了这么多天班了 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);</script></div></footer></div><script>if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }</script><script src=/lib/jquery/index.js></script><script src=/lib/velocity/velocity.min.js></script><script src=/lib/velocity/velocity.ui.min.js></script><script src=/js/src/utils.js></script><script src=/js/src/motion.js></script><script src=/js/src/affix.js></script><script src=/js/src/schemes/pisces.js></script><script src=/js/src/bootstrap.js></script><script async id=dsq-count-scr src=https://johnnyzhuswebsite.disqus.com/count.js></script><script>// Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('-1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });</script><script type=text/x-mathjax-config>
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script><script type=text/x-mathjax-config>
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script><script src=https://cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML></script>